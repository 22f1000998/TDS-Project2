# -*- coding: utf-8 -*-
"""TDS project2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cToWkjQvLnJUlhH90wdyC8BmERiNuTrW
"""

import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import requests
import json

API_URL = "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"
AIPROXY_TOKEN = "eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIyZjEwMDA5OThAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.tcy3aOGcHEi5oJ4M3-M1vO2rqsGS4XkNk2umtd0k3zE"

def load_data(filename):
    encodings = ['utf-8', 'ISO-8859-1']
    for encoding in encodings:
        try:
            df = pd.read_csv(filename, encoding=encoding)
            print(f"File loaded successfully using {encoding} encoding.")
            return df
        except UnicodeDecodeError:
            print(f"{encoding} decoding failed. Trying the next encoding...")
        except FileNotFoundError:
            print(f"Error: File '{filename}' not found.")
            return None
        except PermissionError:
            print(f"Error: Permission denied for file '{filename}'.")
            return None
        except Exception as e:
            print(f"Error loading file with {encoding} encoding: {e}")
    print(f"Failed to load the file '{filename}' with all attempted encodings.")
    return None

def analyze_data(df,include_descriptive_stats=True):
    analysis = {
        "columns": list(df.columns),
        "types": {col: str(dtype) for col, dtype in df.dtypes.items()},
        "missing_values": df.isnull().sum().to_dict(),
    }

    if include_descriptive_stats:
        # Compute summary statistics
        summary_stats = df.describe(include="all").to_dict()
        for col, stats in summary_stats.items():
            for stat, value in stats.items():
                if pd.isnull(value):
                    summary_stats[col][stat] = None
        analysis["summary_stats"] = summary_stats

    # Compute correlation matrix (only for numeric columns)
    numeric_cols = df.select_dtypes(include=["number"])
    if not numeric_cols.empty:
        correlation_matrix = numeric_cols.corr().fillna(0).to_dict()
        analysis["correlation_matrix"] = correlation_matrix
    else:
        analysis["correlation_matrix"] = "No numeric columns for correlation analysis."

    return analysis

def visualize_data(df, file_prefix):
    sns.set(style="whitegrid")
    numeric_columns = df.select_dtypes(include=['number']).columns

    if numeric_columns.empty:
        print("No numeric columns found. No visualizations generated.")
        return

    for column in numeric_columns:
        plt.figure()
        sns.histplot(df[column].dropna(), kde=True)
        plt.title(f'Distribution of {column}')
        plt.savefig(f'{file_prefix}_{column}_distribution.png')
        plt.close()

def query_llm(prompt, proxy_url, headers, model="gpt-4o-mini"):
    try:
        # Construct the payload
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}]
        }

        # Make the POST request
        response = requests.post(proxy_url, headers=headers, json=payload)

        # Handle the response
        if response.status_code == 200:
            result = response.json()
            return result.get("choices", [{}])[0].get("message", {}).get("content", "No content returned").strip()
        else:
            print(f"Error: HTTP {response.status_code} - {response.text}")
            return f"Error querying LLM: HTTP {response.status_code}"
    except requests.exceptions.RequestException as req_err:
        print(f"Request error: {req_err}")
        return "Error querying LLM: Request exception."
    except Exception as e:
        print(f"Unexpected error querying LLM: {e}")
        return "Error querying LLM: Unexpected exception."

def generate_markdown_story(df_summary, output_dir):
    # Create the prompt
    prompt = (
        "Write a detailed Markdown narrative based on the following data analysis summary. "
        "Include key insights, trends, correlations, and highlight missing data with possible explanations:\n\n"
        f"{json.dumps(df_summary, indent=2)}"
    )

    # Generate narrative using the LLM
    narrative = query_llm(prompt)

    # Save the narrative to a Markdown file
    with open(f"{output_dir}/README.md", "w") as file:
        file.write(narrative)

def run_analysis(input_file):

    if not os.path.exists(input_file):
        print(f"File not found: {input_file}")
        return

    try:
        # Prepare output directory
        dataset_name = os.path.splitext(os.path.basename(input_file))[0]
        output_dir = dataset_name

        # Load data
        df = load_data(input_file)
        if df is None:
            return

        # Analyze data and generate outputs
        summary = analyze_data(df)
        visualize_data(df, output_dir)
        generate_markdown_story(summary, output_dir)

        print(f"Analysis complete. Outputs saved in '{output_dir}/'")
    except Exception as e:
        print(f"An error occurred: {e}")

input_file = input("Enter the path to your dataset file (e.g., dataset.csv): ")
run_analysis(input_file)